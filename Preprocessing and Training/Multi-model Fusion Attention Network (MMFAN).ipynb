{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import soundfile as sf\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Flatten, Input, Bidirectional, Concatenate\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "tf.random.set_seed(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and preprocess\n",
    "def load_data(directory, csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    file_paths = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        subdir = f\"{row['segment_id'][:15]}\"\n",
    "        file_paths.append(os.path.join(directory, subdir, 'WAV_' + f\"{row['segment_id']}.wav\").replace('\\\\', '/'))\n",
    "\n",
    "    labels = df['emotion'].values\n",
    "    return file_paths, labels\n",
    "\n",
    "train_paths, train_labels = load_data('./data/KEMDy20ERCNew/train', './data/KEMDy20ERCNew/train_labels.csv')\n",
    "val_paths, val_labels = load_data('./data/KEMDy20ERCNew/val', './data/KEMDy20ERCNew/val_labels.csv')\n",
    "test_paths, test_labels = load_data('./data/KEMDy20ERCNew/test', './data/KEMDy20ERCNew/test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Label Encoding (train_labels, val_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "all_labels = np.concatenate((train_labels, val_labels, test_labels), axis=0)\n",
    "\n",
    "encoder.fit(all_labels)\n",
    "\n",
    "train_labels = encoder.transform(train_labels)\n",
    "val_labels = encoder.transform(val_labels)\n",
    "test_labels = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature extraction using Wav2Vec2\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "def pad_sequences(sequences, max_len=None, padding_value=0.0):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    padded_sequences = np.full((len(sequences), max_len, sequences[0].shape[1]), padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_sequences[i, :len(seq)] = seq\n",
    "    \n",
    "    return padded_sequences\n",
    "\n",
    "def extract_features(file_paths):\n",
    "    features = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        speech, sample_rate = torchaudio.load(file_path)\n",
    "        speech = speech.squeeze(0)  # Remove the unnecessary channel dimension\n",
    "        inputs = processor(speech, return_tensors=\"pt\", padding=True, sampling_rate=sample_rate)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits.squeeze(0).cpu().numpy()  \n",
    "            features.append(logits)\n",
    "    \n",
    "    return pad_sequences(features)  # Use pad_sequences instead of np.stack\n",
    "\n",
    "train_features = extract_features(train_paths)\n",
    "val_features = extract_features(val_paths)\n",
    "test_features = extract_features(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_timesteps = min(train_features.shape[1], val_features.shape[1], test_features.shape[1])\n",
    "\n",
    "train_features = train_features[:, :min_timesteps, :]\n",
    "val_features = val_features[:, :min_timesteps, :]\n",
    "test_features = test_features[:, :min_timesteps, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape, val_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_txt_data(folder_path):\n",
    "    txt_data = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(os.path.join(root, file), \"r\", encoding=\"cp949\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    for line in lines:\n",
    "                        txt_data.append(line.strip())\n",
    "    return txt_data\n",
    "\n",
    "train_text_data = load_txt_data('./data/KEMDy20ERCNew/train')\n",
    "val_text_data = load_txt_data('./data/KEMDy20ERCNew/val')\n",
    "test_text_data = load_txt_data('./data/KEMDy20ERCNew/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa sub-model for text data\n",
    "roberta_model = TFRobertaModel.from_pretrained(\"roberta-large\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "MAX_LENGTH = 128  # Adjust this value according to your GPU memory capacity\n",
    "\n",
    "# Encode the text data using the RoBERTa tokenizer\n",
    "train_input_ids = tokenizer(train_text_data, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"tf\")[\"input_ids\"]\n",
    "val_input_ids = tokenizer(val_text_data, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"tf\")[\"input_ids\"]\n",
    "test_input_ids = tokenizer(test_text_data, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"tf\")[\"input_ids\"]\n",
    "\n",
    "# Freeze the RoBERTa layers\n",
    "for layer in roberta_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Bio-signal (EDA, TEMP)\n",
    "- Extract the features using tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import extract_features\n",
    "\n",
    "train_folder = \"./data/KEMDy20ERCNew/train\"\n",
    "\n",
    "train_eda_data = []\n",
    "train_temp_data = []\n",
    "\n",
    "id_counter = 0\n",
    "\n",
    "for script_folder in os.listdir(train_folder):\n",
    "    script_path = os.path.join(train_folder, script_folder)\n",
    "    if os.path.isdir(script_path):\n",
    "        for csv_file in os.listdir(script_path):\n",
    "            if \"EDA\" in csv_file:\n",
    "                eda_csv_path = os.path.join(script_path, csv_file)\n",
    "                eda_df = pd.read_csv(eda_csv_path)\n",
    "                eda_df[\"id\"] = id_counter\n",
    "                eda_df[\"time\"] = eda_df.index\n",
    "                train_eda_data.append(eda_df[[\"id\", \"time\", \"eda_value\"]])\n",
    "                id_counter += 1\n",
    "            elif \"TEMP\" in csv_file:\n",
    "                temp_csv_path = os.path.join(script_path, csv_file)\n",
    "                temp_df = pd.read_csv(temp_csv_path)\n",
    "                temp_df[\"id\"] = id_counter\n",
    "                temp_df[\"time\"] = temp_df.index\n",
    "                train_temp_data.append(temp_df[[\"id\", \"time\", \"temp_value\"]])\n",
    "                id_counter += 1\n",
    "\n",
    "# Concatenate all the EDA and Temp data\n",
    "train_eda_data = pd.concat(train_eda_data)\n",
    "train_temp_data = pd.concat(train_temp_data)\n",
    "\n",
    "# Extract features using tsfresh\n",
    "train_eda_features = extract_features(train_eda_data, column_id=\"id\", column_sort=\"time\")\n",
    "train_temp_features = extract_features(train_temp_data, column_id=\"id\", column_sort=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_folder = \"./data/KEMDy20ERCNew/val\"\n",
    "\n",
    "val_eda_data = []\n",
    "val_temp_data = []\n",
    "\n",
    "id_counter = 0\n",
    "\n",
    "for script_folder in os.listdir(val_folder):\n",
    "    script_path = os.path.join(val_folder, script_folder)\n",
    "    if os.path.isdir(script_path):\n",
    "        for csv_file in os.listdir(script_path):\n",
    "            if \"EDA\" in csv_file:\n",
    "                eda_csv_path = os.path.join(script_path, csv_file)\n",
    "                eda_df = pd.read_csv(eda_csv_path)\n",
    "                eda_df[\"id\"] = id_counter\n",
    "                eda_df[\"time\"] = eda_df.index\n",
    "                val_eda_data.append(eda_df[[\"id\", \"time\", \"eda_value\"]])\n",
    "                id_counter += 1\n",
    "            elif \"TEMP\" in csv_file:\n",
    "                temp_csv_path = os.path.join(script_path, csv_file)\n",
    "                temp_df = pd.read_csv(temp_csv_path)\n",
    "                temp_df[\"id\"] = id_counter\n",
    "                temp_df[\"time\"] = temp_df.index\n",
    "                val_temp_data.append(temp_df[[\"id\", \"time\", \"temp_value\"]])\n",
    "                id_counter += 1\n",
    "\n",
    "# Concatenate all the EDA and Temp data\n",
    "val_eda_data = pd.concat(val_eda_data)\n",
    "val_temp_data = pd.concat(val_temp_data)\n",
    "\n",
    "# Extract features using tsfresh\n",
    "val_eda_features = extract_features(val_eda_data, column_id=\"id\", column_sort=\"time\")\n",
    "val_temp_features = extract_features(val_temp_data, column_id=\"id\", column_sort=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = \"./data/KEMDy20ERCNew/test\"\n",
    "\n",
    "test_eda_data = []\n",
    "test_temp_data = []\n",
    "\n",
    "id_counter = 0\n",
    "\n",
    "for script_folder in os.listdir(test_folder):\n",
    "    script_path = os.path.join(test_folder, script_folder)\n",
    "    if os.path.isdir(script_path):\n",
    "        for csv_file in os.listdir(script_path):\n",
    "            if \"EDA\" in csv_file:\n",
    "                eda_csv_path = os.path.join(script_path, csv_file)\n",
    "                eda_df = pd.read_csv(eda_csv_path)\n",
    "                eda_df[\"id\"] = id_counter\n",
    "                eda_df[\"time\"] = eda_df.index\n",
    "                test_eda_data.append(eda_df[[\"id\", \"time\", \"eda_value\"]])\n",
    "                id_counter += 1\n",
    "            elif \"TEMP\" in csv_file:\n",
    "                temp_csv_path = os.path.join(script_path, csv_file)\n",
    "                temp_df = pd.read_csv(temp_csv_path)\n",
    "                temp_df[\"id\"] = id_counter\n",
    "                temp_df[\"time\"] = temp_df.index\n",
    "                test_temp_data.append(temp_df[[\"id\", \"time\", \"temp_value\"]])\n",
    "                id_counter += 1\n",
    "\n",
    "# Concatenate all the EDA and Temp data\n",
    "test_eda_data = pd.concat(test_eda_data)\n",
    "test_temp_data = pd.concat(test_temp_data)\n",
    "\n",
    "# Extract features using tsfresh\n",
    "test_eda_features = extract_features(test_eda_data, column_id=\"id\", column_sort=\"time\")\n",
    "test_temp_features = extract_features(test_temp_data, column_id=\"id\", column_sort=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "train_eda_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train_temp_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill missing values using the mean value of the column\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "train_eda_features_filled = imputer.fit_transform(train_eda_features)\n",
    "train_temp_features_filled = imputer.fit_transform(train_temp_features)\n",
    "\n",
    "# Normalize data to the range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "train_eda_features_norm = scaler.fit_transform(train_eda_features_filled)\n",
    "train_temp_features_norm = scaler.fit_transform(train_temp_features_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eda_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "val_temp_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill missing values using the mean value of the column\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "val_eda_features_filled = imputer.fit_transform(val_eda_features)\n",
    "val_temp_features_filled = imputer.fit_transform(val_temp_features)\n",
    "\n",
    "# Normalize data to the range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "val_eda_features_norm = scaler.fit_transform(val_eda_features_filled)\n",
    "val_temp_features_norm = scaler.fit_transform(val_temp_features_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eda_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test_temp_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill missing values using the mean value of the column\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "test_eda_features_filled = imputer.fit_transform(test_eda_features)\n",
    "test_temp_features_filled = imputer.fit_transform(test_temp_features)\n",
    "\n",
    "# Normalize data to the range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "test_eda_features_norm = scaler.fit_transform(test_eda_features_filled)\n",
    "test_temp_features_norm = scaler.fit_transform(test_temp_features_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borderline SMOTE for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "text_smote = BorderlineSMOTE(k_neighbors=5, random_state=2023)\n",
    "audio_smote = BorderlineSMOTE(k_neighbors=5, random_state=2023)\n",
    "bio_smote = BorderlineSMOTE(k_neighbors=5, random_state=2023)\n",
    "\n",
    "text_train_resampled, labels_train_resampled = text_smote.fit_resample(train_input_ids, train_labels)\n",
    "\n",
    "# Reshape audio data to 2D\n",
    "train_features_2d = train_features.reshape(train_features.shape[0], -1)\n",
    "\n",
    "# Apply Borderline SMOTE\n",
    "audio_train_resampled, _ = audio_smote.fit_resample(train_features_2d, train_labels)\n",
    "\n",
    "# Reshape audio data back to original 3D shape\n",
    "audio_train_resampled = audio_train_resampled.reshape(-1, train_features.shape[1], train_features.shape[2])\n",
    "\n",
    "# Apply Borderline SMOTE on bio signal data\n",
    "eda_train_resampled, _ = bio_smote.fit_resample(train_eda_features_norm, train_labels)\n",
    "temp_train_resampled, _ = bio_smote.fit_resample(train_temp_features_norm, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to have a time dimension\n",
    "eda_train_resampled_3d = np.expand_dims(eda_train_resampled, axis=1)\n",
    "temp_train_resampled_3d = np.expand_dims(temp_train_resampled, axis=1)\n",
    "\n",
    "val_eda_features_3d = np.expand_dims(val_eda_features_norm, axis=1)\n",
    "val_temp_features_3d = np.expand_dims(val_temp_features_norm, axis=1)\n",
    "\n",
    "test_eda_features_3d = np.expand_dims(test_eda_features_norm, axis=1)\n",
    "test_temp_features_3d = np.expand_dims(test_temp_features_norm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum length of EDA signals across all datasets\n",
    "ead_min_len = min(eda_train_resampled_3d.shape[2], val_eda_features_3d.shape[2], test_eda_features_3d.shape[2])\n",
    "\n",
    "# Trim the EDA signals to the minimum length\n",
    "eda_train_resampled_3d = eda_train_resampled_3d[:, :, :ead_min_len]\n",
    "val_eda_features_3d = val_eda_features_3d[:, :, :ead_min_len]\n",
    "test_eda_features_3d = test_eda_features_3d[:, :, :ead_min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum length of EDA signals across all datasets\n",
    "temp_min_len = min(temp_train_resampled_3d.shape[2], val_temp_features_3d.shape[2], test_temp_features_3d.shape[2])\n",
    "\n",
    "# Trim the EDA signals to the minimum length\n",
    "temp_train_resampled_3d = temp_train_resampled_3d[:, :, :temp_min_len]\n",
    "val_temp_features_3d = val_temp_features_3d[:, :, :temp_min_len]\n",
    "test_temp_features_3d = test_temp_features_3d[:, :, :temp_min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train_resampled_3d.shape, val_temp_features_3d.shape, test_temp_features_3d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bio-signal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eda_model(eda_input_shape):\n",
    "    eda_input = Input(shape=eda_input_shape)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(eda_input)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, activation=tf.nn.gelu)(x)\n",
    "\n",
    "    return Model(inputs=eda_input, outputs=x)\n",
    "\n",
    "def create_temp_model(temp_input_shape):\n",
    "    temp_input = Input(shape=temp_input_shape)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(temp_input)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, activation=tf.nn.gelu)(x)\n",
    "\n",
    "    return Model(inputs=temp_input, outputs=x)\n",
    "\n",
    "# Update the input shapes\n",
    "eda_input_shape = (1, eda_train_resampled_3d.shape[2])\n",
    "temp_input_shape = (1, temp_train_resampled_3d.shape[2])\n",
    "\n",
    "eda_model = create_eda_model(eda_input_shape)\n",
    "temp_model = create_temp_model(temp_input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_roberta_text_model():\n",
    "    text_input_ids = Input(shape=(MAX_LENGTH,), dtype=tf.int32)\n",
    "    roberta_output = roberta_model(text_input_ids)\n",
    "    roberta_embeddings = roberta_output[0][:, 0, :]\n",
    "    text_roberta = Dense(32, activation=tf.nn.gelu)(roberta_embeddings)\n",
    "    return tf.keras.Model(inputs=text_input_ids, outputs=text_roberta)\n",
    "\n",
    "text_model = create_roberta_text_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_input_shape = (train_features.shape[1], train_features.shape[2])\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=audio_input_shape),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation=tf.nn.gelu)\n",
    "])\n",
    "\n",
    "# MLP-Mixer Model\n",
    "from tensorflow.keras.layers import Layer, Add\n",
    "class MixerLayer(Layer):\n",
    "    def __init__(self, tokens_mlp_dim, channels_mlp_dim):\n",
    "        super().__init__()\n",
    "        self.tokens_mlp_dim = tokens_mlp_dim\n",
    "        self.channels_mlp_dim = channels_mlp_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dense1 = Dense(self.tokens_mlp_dim, activation=tf.nn.gelu)\n",
    "        self.dense2 = Dense(input_shape[1], activation=tf.nn.gelu)\n",
    "        self.dense3 = Dense(self.channels_mlp_dim, activation=tf.nn.gelu)\n",
    "        self.dense4 = Dense(input_shape[2], activation=tf.nn.gelu)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Token mixing\n",
    "        x = self.layer_norm1(inputs)\n",
    "        x_t = tf.transpose(x, perm=[0, 2, 1])\n",
    "        x_t = self.dense1(x_t)\n",
    "        x_t = self.dense2(x_t)\n",
    "        x_t = tf.transpose(x_t, perm=[0, 2, 1])\n",
    "        x = Add()([x, x_t])\n",
    "\n",
    "        # Channel mixing\n",
    "        y = self.layer_norm2(x)\n",
    "        y = self.dense3(y)\n",
    "        y = self.dense4(y)\n",
    "        y = Add()([x, y])\n",
    "\n",
    "        return tf.keras.layers.Flatten()(y)  # Add Flatten layer here\n",
    "    \n",
    "# MLP-Mixer model\n",
    "mlp_mixer_model = Sequential([\n",
    "    Input(shape=audio_input_shape),\n",
    "    MixerLayer(tokens_mlp_dim=64, channels_mlp_dim=64),\n",
    "    Flatten(),\n",
    "    Dense(32, activation=tf.nn.gelu)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Model (Audio+Text+Bio-signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModelWithTextAndBioSignals(tf.keras.Model):\n",
    "    def __init__(self, text_model, lstm_model, mlp_mixer_model, eda_model, temp_model):\n",
    "        super(CombinedModelWithTextAndBioSignals, self).__init__()\n",
    "        self.text_model = text_model\n",
    "        self.lstm_model = lstm_model\n",
    "        self.mlp_mixer_model = mlp_mixer_model\n",
    "        self.eda_model = eda_model\n",
    "        self.temp_model = temp_model\n",
    "        \n",
    "        self.combine_layer = Dense(64, activation=tf.nn.gelu)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads=4, key_dim=32)\n",
    "        self.classifier = Dense(7, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        text_input, audio_input, eda_input, temp_input = inputs\n",
    "        text_output = self.text_model(text_input)\n",
    "        lstm_output = self.lstm_model(audio_input)\n",
    "        mlp_mixer_output = self.mlp_mixer_model(audio_input)\n",
    "        eda_output = self.eda_model(eda_input)\n",
    "        temp_output = self.temp_model(temp_input)\n",
    "        x = tf.concat([text_output, lstm_output, mlp_mixer_output, eda_output, temp_output], axis=-1)\n",
    "        x = self.combine_layer(x)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.expand_dims(x, axis=1)  # Add sequence dimension\n",
    "        x = self.multihead_attention(query=x, key=x, value=x)\n",
    "        x = tf.squeeze(x, axis=1)  # Remove the sequence dimension \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "combined_model_with_text_and_bio_signals = CombinedModelWithTextAndBioSignals(text_model, lstm_model, mlp_mixer_model, eda_model, temp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, model_name):\n",
    "    \n",
    "    # Set the learning rate for the Adam optimizer\n",
    "    learning_rate = 5e-4  # Change this value to adjust the learning rate\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(f\"{model_name}_best_weights.h5\", monitor='val_loss', \n",
    "                                                             save_best_only=True, mode='min', save_weights_only=True)\n",
    "\n",
    "    model.fit((text_train_resampled, audio_train_resampled, eda_train_resampled_3d, temp_train_resampled_3d), labels_train_resampled, epochs=200, \n",
    "              validation_data=((val_input_ids, val_features, val_eda_features_3d, val_temp_features_3d), val_labels), batch_size=32, callbacks=[checkpoint_callback])\n",
    "\n",
    "    model.load_weights(f\"{model_name}_best_weights.h5\")\n",
    "    predictions = model.predict((val_input_ids, val_features, val_eda_features_3d, val_temp_features_3d))\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(classification_report(val_labels, predictions))\n",
    "    print(f\"{model_name} - Confusion Matrix:\")\n",
    "    print(confusion_matrix(val_labels, predictions))\n",
    "\n",
    "# Train and evaluate the combined model with text and bio signals\n",
    "train_and_evaluate(combined_model_with_text_and_bio_signals, \"CombinedModelWithTextAndBioSignals+Att-230413\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = combined_model_with_text_and_bio_signals.predict((val_input_ids, val_features, val_eda_features_3d, val_temp_features_3d))\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(\"Audio_Text_BioSignal Classification Report:\")\n",
    "print(classification_report(val_labels, predictions))\n",
    "print(\"Audio_Text_BioSignal Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = combined_model_with_text_and_bio_signals.predict((test_input_ids, test_features, test_eda_features_3d, test_temp_features_3d))\n",
    "test_predictions = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "print(\"Audio_Text_BioSignal - Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "print(\"Audio_Text_BioSignal - Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(round(f1_score(test_labels, test_predictions, average='weighted'), 4))\n",
    "print(round(f1_score(test_labels, test_predictions, average='micro'), 4))\n",
    "print(round(f1_score(test_labels, test_predictions, average='macro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, title='Confusion Matrix'):\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matrix, annot=True, cmap='Blues', fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_percent(y_true, y_pred, labels, title='Confusion Matrix', cmap='Blues'):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm_percentage = cm_normalized * 100\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap=cmap, xticklabels=labels, yticklabels=labels)\n",
    "    \n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "plot_confusion_matrix(test_labels, test_predictions, labels)\n",
    "plot_confusion_matrix_percent(test_labels, test_predictions, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
