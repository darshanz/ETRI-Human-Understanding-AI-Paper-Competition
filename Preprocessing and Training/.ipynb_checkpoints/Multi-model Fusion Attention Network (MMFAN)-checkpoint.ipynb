{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import soundfile as sf\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Flatten, Input, Bidirectional, Concatenate\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "tf.random.set_seed(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and preprocess\n",
    "def load_data(directory, csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    file_paths = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        subdir = f\"{row['segment_id'][:15]}\"\n",
    "        file_paths.append(os.path.join(directory, subdir, 'WAV_' + f\"{row['segment_id']}.wav\").replace('\\\\', '/'))\n",
    "\n",
    "    labels = df['emotion'].values\n",
    "    return file_paths, labels\n",
    "\n",
    "train_paths, train_labels = load_data('./data/KEMDy20ERCNew/train', './data/KEMDy20ERCNew/train_labels.csv')\n",
    "val_paths, val_labels = load_data('./data/KEMDy20ERCNew/val', './data/KEMDy20ERCNew/val_labels.csv')\n",
    "test_paths, test_labels = load_data('./data/KEMDy20ERCNew/test', './data/KEMDy20ERCNew/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 레이블 인코더 객체 생성\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# 레이블 인코딩을 위한 데이터 합치기\n",
    "all_labels = np.concatenate((train_labels, val_labels, test_labels), axis=0)\n",
    "\n",
    "# 모든 레이블에 대해 인코더를 학습\n",
    "encoder.fit(all_labels)\n",
    "\n",
    "# train_labels, val_labels, test_labels를 정수 형태의 레이블로 변환\n",
    "train_labels = encoder.transform(train_labels)\n",
    "val_labels = encoder.transform(val_labels)\n",
    "test_labels = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature extraction using Wav2Vec2\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "def pad_sequences(sequences, max_len=None, padding_value=0.0):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    padded_sequences = np.full((len(sequences), max_len, sequences[0].shape[1]), padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_sequences[i, :len(seq)] = seq\n",
    "    \n",
    "    return padded_sequences\n",
    "\n",
    "def extract_features(file_paths):\n",
    "    features = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        speech, sample_rate = torchaudio.load(file_path)\n",
    "        speech = speech.squeeze(0)  # Remove the unnecessary channel dimension\n",
    "        inputs = processor(speech, return_tensors=\"pt\", padding=True, sampling_rate=sample_rate)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits.squeeze(0).cpu().numpy()  \n",
    "            features.append(logits)\n",
    "    \n",
    "    return pad_sequences(features)  # Use pad_sequences instead of np.stack\n",
    "\n",
    "train_features = extract_features(train_paths)\n",
    "val_features = extract_features(val_paths)\n",
    "test_features = extract_features(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_features.npy\", train_features)\n",
    "np.save(\"val_features.npy\", val_features)\n",
    "np.save(\"test_features.npy\", test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.load(\"train_features.npy\")\n",
    "val_features = np.load(\"val_features.npy\")\n",
    "test_features = np.load(\"test_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape, val_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_txt_data(folder_path):\n",
    "    txt_data = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(os.path.join(root, file), \"r\", encoding=\"cp949\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    for line in lines:\n",
    "                        txt_data.append(line.strip())\n",
    "    return txt_data\n",
    "\n",
    "train_text_data = load_txt_data('./data/KEMDy20ERCNew/train')\n",
    "val_text_data = load_txt_data('./data/KEMDy20ERCNew/val')\n",
    "test_text_data = load_txt_data('./data/KEMDy20ERCNew/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa sub-model for text data\n",
    "roberta_model = TFRobertaModel.from_pretrained(\"roberta-large\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "MAX_LENGTH = 128  # Adjust this value according to your GPU memory capacity\n",
    "\n",
    "# Encode the text data using the RoBERTa tokenizer\n",
    "train_input_ids = tokenizer(train_text_data, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"tf\")[\"input_ids\"]\n",
    "val_input_ids = tokenizer(val_text_data, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"tf\")[\"input_ids\"]\n",
    "test_input_ids = tokenizer(test_text_data, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"tf\")[\"input_ids\"]\n",
    "\n",
    "# Freeze the RoBERTa layers\n",
    "for layer in roberta_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bio-signal (EDA, TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import extract_features\n",
    "\n",
    "train_folder = \"./data/KEMDy20ERCNew/train\"\n",
    "\n",
    "train_eda_data = []\n",
    "train_temp_data = []\n",
    "\n",
    "id_counter = 0\n",
    "\n",
    "for script_folder in os.listdir(train_folder):\n",
    "    script_path = os.path.join(train_folder, script_folder)\n",
    "    if os.path.isdir(script_path):\n",
    "        for csv_file in os.listdir(script_path):\n",
    "            if \"EDA\" in csv_file:\n",
    "                eda_csv_path = os.path.join(script_path, csv_file)\n",
    "                eda_df = pd.read_csv(eda_csv_path)\n",
    "                eda_df[\"id\"] = id_counter\n",
    "                eda_df[\"time\"] = eda_df.index\n",
    "                train_eda_data.append(eda_df[[\"id\", \"time\", \"eda_value\"]])\n",
    "                id_counter += 1\n",
    "            elif \"TEMP\" in csv_file:\n",
    "                temp_csv_path = os.path.join(script_path, csv_file)\n",
    "                temp_df = pd.read_csv(temp_csv_path)\n",
    "                temp_df[\"id\"] = id_counter\n",
    "                temp_df[\"time\"] = temp_df.index\n",
    "                train_temp_data.append(temp_df[[\"id\", \"time\", \"temp_value\"]])\n",
    "                id_counter += 1\n",
    "\n",
    "# Concatenate all the EDA and Temp data\n",
    "train_eda_data = pd.concat(train_eda_data)\n",
    "train_temp_data = pd.concat(train_temp_data)\n",
    "\n",
    "# Extract features using tsfresh\n",
    "train_eda_features = extract_features(train_eda_data, column_id=\"id\", column_sort=\"time\")\n",
    "train_temp_features = extract_features(train_temp_data, column_id=\"id\", column_sort=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eda_features.shape, train_temp_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_folder = \"./data/KEMDy20ERCNew/val\"\n",
    "\n",
    "val_eda_data = []\n",
    "val_temp_data = []\n",
    "\n",
    "id_counter = 0\n",
    "\n",
    "for script_folder in os.listdir(val_folder):\n",
    "    script_path = os.path.join(val_folder, script_folder)\n",
    "    if os.path.isdir(script_path):\n",
    "        for csv_file in os.listdir(script_path):\n",
    "            if \"EDA\" in csv_file:\n",
    "                eda_csv_path = os.path.join(script_path, csv_file)\n",
    "                eda_df = pd.read_csv(eda_csv_path)\n",
    "                eda_df[\"id\"] = id_counter\n",
    "                eda_df[\"time\"] = eda_df.index\n",
    "                val_eda_data.append(eda_df[[\"id\", \"time\", \"eda_value\"]])\n",
    "                id_counter += 1\n",
    "            elif \"TEMP\" in csv_file:\n",
    "                temp_csv_path = os.path.join(script_path, csv_file)\n",
    "                temp_df = pd.read_csv(temp_csv_path)\n",
    "                temp_df[\"id\"] = id_counter\n",
    "                temp_df[\"time\"] = temp_df.index\n",
    "                val_temp_data.append(temp_df[[\"id\", \"time\", \"temp_value\"]])\n",
    "                id_counter += 1\n",
    "\n",
    "# Concatenate all the EDA and Temp data\n",
    "val_eda_data = pd.concat(val_eda_data)\n",
    "val_temp_data = pd.concat(val_temp_data)\n",
    "\n",
    "# Extract features using tsfresh\n",
    "val_eda_features = extract_features(val_eda_data, column_id=\"id\", column_sort=\"time\")\n",
    "val_temp_features = extract_features(val_temp_data, column_id=\"id\", column_sort=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = \"./data/KEMDy20ERCNew/test\"\n",
    "\n",
    "test_eda_data = []\n",
    "test_temp_data = []\n",
    "\n",
    "id_counter = 0\n",
    "\n",
    "for script_folder in os.listdir(test_folder):\n",
    "    script_path = os.path.join(test_folder, script_folder)\n",
    "    if os.path.isdir(script_path):\n",
    "        for csv_file in os.listdir(script_path):\n",
    "            if \"EDA\" in csv_file:\n",
    "                eda_csv_path = os.path.join(script_path, csv_file)\n",
    "                eda_df = pd.read_csv(eda_csv_path)\n",
    "                eda_df[\"id\"] = id_counter\n",
    "                eda_df[\"time\"] = eda_df.index\n",
    "                test_eda_data.append(eda_df[[\"id\", \"time\", \"eda_value\"]])\n",
    "                id_counter += 1\n",
    "            elif \"TEMP\" in csv_file:\n",
    "                temp_csv_path = os.path.join(script_path, csv_file)\n",
    "                temp_df = pd.read_csv(temp_csv_path)\n",
    "                temp_df[\"id\"] = id_counter\n",
    "                temp_df[\"time\"] = temp_df.index\n",
    "                test_temp_data.append(temp_df[[\"id\", \"time\", \"temp_value\"]])\n",
    "                id_counter += 1\n",
    "\n",
    "# Concatenate all the EDA and Temp data\n",
    "test_eda_data = pd.concat(test_eda_data)\n",
    "test_temp_data = pd.concat(test_temp_data)\n",
    "\n",
    "# Extract features using tsfresh\n",
    "test_eda_features = extract_features(test_eda_data, column_id=\"id\", column_sort=\"time\")\n",
    "test_temp_features = extract_features(test_temp_data, column_id=\"id\", column_sort=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "train_eda_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train_temp_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill missing values using the mean value of the column\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "train_eda_features_filled = imputer.fit_transform(train_eda_features)\n",
    "train_temp_features_filled = imputer.fit_transform(train_temp_features)\n",
    "\n",
    "# Normalize data to the range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "train_eda_features_norm = scaler.fit_transform(train_eda_features_filled)\n",
    "train_temp_features_norm = scaler.fit_transform(train_temp_features_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eda_features_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eda_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "val_temp_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill missing values using the mean value of the column\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "val_eda_features_filled = imputer.fit_transform(val_eda_features)\n",
    "val_temp_features_filled = imputer.fit_transform(val_temp_features)\n",
    "\n",
    "# Normalize data to the range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "val_eda_features_norm = scaler.fit_transform(val_eda_features_filled)\n",
    "val_temp_features_norm = scaler.fit_transform(val_temp_features_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eda_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test_temp_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill missing values using the mean value of the column\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "test_eda_features_filled = imputer.fit_transform(test_eda_features)\n",
    "test_temp_features_filled = imputer.fit_transform(test_temp_features)\n",
    "\n",
    "# Normalize data to the range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "test_eda_features_norm = scaler.fit_transform(test_eda_features_filled)\n",
    "test_temp_features_norm = scaler.fit_transform(test_temp_features_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borderline SMOTE for audio and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "text_smote = BorderlineSMOTE(k_neighbors=5, random_state=2023)\n",
    "audio_smote = BorderlineSMOTE(k_neighbors=5, random_state=2023)\n",
    "bio_smote = BorderlineSMOTE(k_neighbors=5, random_state=2023)\n",
    "\n",
    "text_train_resampled, labels_train_resampled = text_smote.fit_resample(train_input_ids, train_labels)\n",
    "\n",
    "# Reshape audio data to 2D\n",
    "train_features_2d = train_features.reshape(train_features.shape[0], -1)\n",
    "\n",
    "# Apply Borderline SMOTE\n",
    "audio_train_resampled, _ = audio_smote.fit_resample(train_features_2d, train_labels)\n",
    "\n",
    "# Reshape audio data back to original 3D shape\n",
    "audio_train_resampled = audio_train_resampled.reshape(-1, train_features.shape[1], train_features.shape[2])\n",
    "\n",
    "# # Apply Borderline SMOTE on bio signal data\n",
    "# eda_train_resampled, _ = bio_smote.fit_resample(train_eda_features_norm, train_labels)\n",
    "# temp_train_resampled, _ = bio_smote.fit_resample(train_temp_features_norm, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"text_train_resampled.npy\", text_train_resampled)\n",
    "np.save(\"labels_train_resampled.npy\", labels_train_resampled)\n",
    "np.save(\"audio_train_resampled.npy\", audio_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_resampled = np.load(\"text_train_resampled.npy\")\n",
    "labels_train_resampled = np.load(\"labels_train_resampled.npy\")\n",
    "audio_train_resampled = np.load(\"audio_train_resampled.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_resampled.shape, labels_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_train_resampled.shape, temp_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to have a time dimension\n",
    "eda_train_resampled_3d = np.expand_dims(eda_train_resampled, axis=1)\n",
    "temp_train_resampled_3d = np.expand_dims(temp_train_resampled, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_train_resampled_3d.shape, temp_train_resampled_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eda_features_3d = np.expand_dims(val_eda_features_norm, axis=1)\n",
    "val_temp_features_3d = np.expand_dims(val_temp_features_norm, axis=1)\n",
    "\n",
    "test_eda_features_3d = np.expand_dims(test_eda_features_norm, axis=1)\n",
    "test_temp_features_3d = np.expand_dims(test_temp_features_norm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eda_features_3d.shape, val_temp_features_3d.shape, test_eda_features_3d.shape, test_temp_features_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"val_eda_features_3d.npy\", val_eda_features_3d)\n",
    "np.save(\"val_temp_features_3d.npy\", val_temp_features_3d)\n",
    "np.save(\"test_eda_features_3d.npy\", test_eda_features_3d)\n",
    "np.save(\"test_temp_features_3d.npy\", test_temp_features_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eda_features_3d = np.load(\"val_eda_features_3d.npy\")\n",
    "val_temp_features_3d = np.load(\"val_temp_features_3d.npy\")\n",
    "test_eda_features_3d = np.load(\"test_eda_features_3d.npy\")\n",
    "test_temp_features_3d = np.load(\"test_temp_features_3d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum length of EDA signals across all datasets\n",
    "ead_min_len = min(eda_train_resampled_3d.shape[2], val_eda_features_3d.shape[2], test_eda_features_3d.shape[2])\n",
    "\n",
    "# Trim the EDA signals to the minimum length\n",
    "eda_train_resampled_3d = eda_train_resampled_3d[:, :, :ead_min_len]\n",
    "val_eda_features_3d = val_eda_features_3d[:, :, :ead_min_len]\n",
    "test_eda_features_3d = test_eda_features_3d[:, :, :ead_min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_train_resampled_3d.shape, val_eda_features_3d.shape, test_eda_features_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"eda_train_resampled_3d.npy\", eda_train_resampled_3d)\n",
    "np.save(\"temp_train_resampled_3d.npy\", temp_train_resampled_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_train_resampled_3d = np.load(\"eda_train_resampled_3d.npy\")\n",
    "temp_train_resampled_3d = np.load(\"temp_train_resampled_3d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum length of EDA signals across all datasets\n",
    "temp_min_len = min(temp_train_resampled_3d.shape[2], val_temp_features_3d.shape[2], test_temp_features_3d.shape[2])\n",
    "\n",
    "# Trim the EDA signals to the minimum length\n",
    "temp_train_resampled_3d = temp_train_resampled_3d[:, :, :temp_min_len]\n",
    "val_temp_features_3d = val_temp_features_3d[:, :, :temp_min_len]\n",
    "test_temp_features_3d = test_temp_features_3d[:, :, :temp_min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train_resampled_3d.shape, val_temp_features_3d.shape, test_temp_features_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eda_model(eda_input_shape):\n",
    "    eda_input = Input(shape=eda_input_shape)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(eda_input)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, activation=tf.nn.gelu)(x)\n",
    "\n",
    "    return Model(inputs=eda_input, outputs=x)\n",
    "\n",
    "def create_temp_model(temp_input_shape):\n",
    "    temp_input = Input(shape=temp_input_shape)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(temp_input)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, activation=tf.nn.gelu)(x)\n",
    "\n",
    "    return Model(inputs=temp_input, outputs=x)\n",
    "\n",
    "# Update the input shapes\n",
    "eda_input_shape = (1, eda_train_resampled_3d.shape[2])\n",
    "temp_input_shape = (1, temp_train_resampled_3d.shape[2])\n",
    "\n",
    "eda_model = create_eda_model(eda_input_shape)\n",
    "temp_model = create_temp_model(temp_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_input_shape, temp_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_roberta_text_model():\n",
    "    text_input_ids = Input(shape=(MAX_LENGTH,), dtype=tf.int32)\n",
    "    roberta_output = roberta_model(text_input_ids)\n",
    "    roberta_embeddings = roberta_output[0][:, 0, :]\n",
    "    text_roberta = Dense(32, activation=tf.nn.gelu)(roberta_embeddings)\n",
    "    return tf.keras.Model(inputs=text_input_ids, outputs=text_roberta)\n",
    "\n",
    "text_model = create_roberta_text_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_input_shape = (train_features.shape[1], train_features.shape[2])\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=audio_input_shape),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation=tf.nn.gelu)\n",
    "])\n",
    "\n",
    "# MLP-Mixer Model\n",
    "from tensorflow.keras.layers import Layer, Add\n",
    "class MixerLayer(Layer):\n",
    "    def __init__(self, tokens_mlp_dim, channels_mlp_dim):\n",
    "        super().__init__()\n",
    "        self.tokens_mlp_dim = tokens_mlp_dim\n",
    "        self.channels_mlp_dim = channels_mlp_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dense1 = Dense(self.tokens_mlp_dim, activation=tf.nn.gelu)\n",
    "        self.dense2 = Dense(input_shape[1], activation=tf.nn.gelu)\n",
    "        self.dense3 = Dense(self.channels_mlp_dim, activation=tf.nn.gelu)\n",
    "        self.dense4 = Dense(input_shape[2], activation=tf.nn.gelu)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Token mixing\n",
    "        x = self.layer_norm1(inputs)\n",
    "        x_t = tf.transpose(x, perm=[0, 2, 1])\n",
    "        x_t = self.dense1(x_t)\n",
    "        x_t = self.dense2(x_t)\n",
    "        x_t = tf.transpose(x_t, perm=[0, 2, 1])\n",
    "        x = Add()([x, x_t])\n",
    "\n",
    "        # Channel mixing\n",
    "        y = self.layer_norm2(x)\n",
    "        y = self.dense3(y)\n",
    "        y = self.dense4(y)\n",
    "        y = Add()([x, y])\n",
    "\n",
    "        return tf.keras.layers.Flatten()(y)  # Add Flatten layer here\n",
    "    \n",
    "# MLP-Mixer model\n",
    "mlp_mixer_model = Sequential([\n",
    "    Input(shape=audio_input_shape),\n",
    "    MixerLayer(tokens_mlp_dim=64, channels_mlp_dim=64),\n",
    "    Flatten(),\n",
    "    Dense(32, activation=tf.nn.gelu)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModelWithTextAndBioSignals(tf.keras.Model):\n",
    "    def __init__(self, text_model, lstm_model, mlp_mixer_model, eda_model, temp_model):\n",
    "        super(CombinedModelWithTextAndBioSignals, self).__init__()\n",
    "        self.text_model = text_model\n",
    "        self.lstm_model = lstm_model\n",
    "        self.mlp_mixer_model = mlp_mixer_model\n",
    "        self.eda_model = eda_model\n",
    "        self.temp_model = temp_model\n",
    "        \n",
    "        self.combine_layer = Dense(64, activation=tf.nn.gelu)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads=4, key_dim=32)\n",
    "        self.classifier = Dense(7, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        text_input, audio_input, eda_input, temp_input = inputs\n",
    "        text_output = self.text_model(text_input)\n",
    "        lstm_output = self.lstm_model(audio_input)\n",
    "        mlp_mixer_output = self.mlp_mixer_model(audio_input)\n",
    "        eda_output = self.eda_model(eda_input)\n",
    "        temp_output = self.temp_model(temp_input)\n",
    "        x = tf.concat([text_output, lstm_output, mlp_mixer_output, eda_output, temp_output], axis=-1)\n",
    "        x = self.combine_layer(x)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.expand_dims(x, axis=1)  # Add sequence dimension\n",
    "        x = self.multihead_attention(query=x, key=x, value=x)\n",
    "        x = tf.squeeze(x, axis=1)  # Remove the sequence dimension \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "combined_model_with_text_and_bio_signals = CombinedModelWithTextAndBioSignals(text_model, lstm_model, mlp_mixer_model, eda_model, temp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, model_name):\n",
    "    \n",
    "    # Set the learning rate for the Adam optimizer\n",
    "    learning_rate = 5e-4  # Change this value to adjust the learning rate\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(f\"{model_name}_best_weights.h5\", monitor='val_loss', \n",
    "                                                             save_best_only=True, mode='min', save_weights_only=True)\n",
    "\n",
    "    model.fit((text_train_resampled, audio_train_resampled, eda_train_resampled_3d, temp_train_resampled_3d), labels_train_resampled, epochs=200, \n",
    "              validation_data=((val_input_ids, val_features, val_eda_features_3d, val_temp_features_3d), val_labels), batch_size=32, callbacks=[checkpoint_callback])\n",
    "\n",
    "    model.load_weights(f\"{model_name}_best_weights.h5\")\n",
    "    predictions = model.predict((val_input_ids, val_features, val_eda_features_3d, val_temp_features_3d))\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(classification_report(val_labels, predictions))\n",
    "    print(f\"{model_name} - Confusion Matrix:\")\n",
    "    print(confusion_matrix(val_labels, predictions))\n",
    "\n",
    "# Train and evaluate the combined model with text and bio signals\n",
    "train_and_evaluate(combined_model_with_text_and_bio_signals, \"CombinedModelWithTextAndBioSignals+Att-230413\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model_with_text_and_bio_signals.load_weights(\"CombinedModelWithTextAndBioSignals+Att-230413_best_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = combined_model_with_text_and_bio_signals.predict((val_input_ids, val_features, val_eda_features_3d, val_temp_features_3d))\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(\"Audio_Text_BioSignal Classification Report:\")\n",
    "print(classification_report(val_labels, predictions))\n",
    "print(\"Audio_Text_BioSignal Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = combined_model_with_text_and_bio_signals.predict((test_input_ids, test_features, test_eda_features_3d, test_temp_features_3d))\n",
    "test_predictions = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "print(\"Audio_Text_BioSignal - Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "print(\"Audio_Text_BioSignal - Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(round(f1_score(test_labels, test_predictions, average='weighted'), 4))\n",
    "print(round(f1_score(test_labels, test_predictions, average='micro'), 4))\n",
    "print(round(f1_score(test_labels, test_predictions, average='macro'), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
